---
title: 阅读笔记 InternLM2 Technical Report
author: adeshen
date: 2024-03-28 18:41:00 -0000
categories: [LLM,InternLM2]
tags: [class,llm]
---
# 书生浦语模型类型

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711541135680-68e3ad57-2fd7-4a07-91ab-bb07c3904ba9.png)

**规模：7B、20B**

 **InternLM2_Base** ：高质量、强可塑性的基座模型

 **InternLM2** ：在Base基础上，在多个能力进行了强化，更优秀的基座模型

 **Chat-SFT** ：在Base基础上，经过有监督微调SFT后的对话模型

 **InternLM2-Chat** ：在Chat-SFT基础上，再经过RLHF对齐后的对话模型

# InternLM亮点

1. 超长上下文，200k token
2. 在数学、代码能力上比肩ChatGPT
3. 具有代码解释器

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711541358122-c73bc1ba-293a-41a6-a89a-31da3a629c91.png)

# 模型到应用

根据业务场景 评估所需要的算力

来决定到底采用续训练/全参数微调，还是部分参数微调

如果需要和环境进行交互，那么则需要构建AI agent


![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711541791476-9a4f75c2-a215-48f7-b31a-d6cfea860eb9.png)

# 开源链条

下面结合InternLM2 技术报告 [arxiv.org/pdf/2403.17297.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2403.17297.pdf)

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711541937553-8f55ca21-7d68-40b0-9a2f-5582f5970f25.png)

## 数据集

**书生万卷1.0   2TB**

**书生万卷CC   400GB**

**开源数据集平台OpenDataLab**：[OpenDataLab](https://link.zhihu.com/?target=https%3A//opendatalab.com/)

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711541244338-3e2597c5-52aa-406b-8a31-289957ec7efc.png)

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711541996168-7b9697c7-b985-411d-9980-37dcb121a6ff.png)

## 数据处理

预训练数据包括**文本数据、代码数据、超长上下文数据**

 **文本数据预处理流程** ：jsonl格式化 --> 去重 --> 去乱码 --> 去有害 --> 去广告 --> 去低质

 **代码数据预处理流程** ：md格式化 --> 去重 --> 去低质 --> 依赖排序

 **超长上下文数据预处理** ：长文本选择 --> 统计过滤 --> 上下文关联性过滤

## 预训练

internEvo： 预训练框架，可以pretrain，SFT,RLHF

[使用教程 — InternEvo 0.3.4 文档](https://internevo.readthedocs.io/zh-cn/latest/usage.html)

支持多种并行训练方式：data，tensor，squence，pipeline

**同时使用了2019的 Zero Redundancy Optimizer 用于减少训练显存开销，此外还有2023 FlashAttention，2023 mixed-precision training 增加硬件的利用率。**

这3篇文章论文的效率提升，正是我目前所需要的。

**微调 Xtuner    适配20系列以上的显卡，只需要8GB显存即可微调7B模型**

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711542109792-8c4659e1-185b-425e-a559-9360dc044383.png)

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711542186384-2289d75f-afc3-425a-892b-07f03058515e.png)

在训练阶段，7B和20B模型分别使用AdamW优化器，初始学习率为4e-5，进行了一个epoch的训练。

在训练步骤中每个微批的每次向前和向后传递期间，InternEvo通过AllGather有效地预取即将到来的层的完整参数集，同时并发地计算当前层。生成的梯度通过ReduceScatter在参数分片组内进行同步，随后使用AllReduce在参数分片组之间进行同步。
这里将分布式通信的过程与训练的过程重叠在一起，最大限度地提高了训练管道的效率。


## 模型架构

总体遵循LLaMA的结构设计原则，沿用了RMSNorm和SwiGLU激活函数，除此之外

1. 对模型中的权重矩阵如Wk, Wq, Wv进行了调整，以支持不同的张量并行转换，并提高训练速度。
2. 为了支持长上下文，采用了分组查询注意力 (GQA)结构，以便在处理非常长的上下文时保持高速和低GPU显存消耗。

## 评测

**openCompass2.0**

**CompassKit 大模型评测全站工具链，可以用于查看大模型能力榜单**

[OpenCompass](https://opencompass.org.cn/home)

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711542257154-1c3612ea-edc5-417b-a9bf-1df49632eac9.png)

**CompassHub 高质量基准数据集，其中评测最高分为gpt4turbo，61.8分。应该算标准比较高的。**

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711542446186-822fc802-42e8-44c2-856c-98a46ae475b2.png)

## 部署

**LMDeploy**

[欢迎来到 LMDeploy 的中文教程！ — lmdeploy 0.2.6 文档](https://lmdeploy.readthedocs.io/zh-cn/latest/index.html)

它提供大模型在GPU上部署的全流程解决方案，包括模型轻量化、推理和服务。

轻量化：支持4bit权重，8bitK/V cache

推理引擎：turbomind、pytorch

服务：openai-sever、gradio、trition inference server

和deepmind有一定差距

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711542718013-9a7c3605-72bd-41d0-97ea-54ad52448032.png)

## 轻量级智能体框架

[欢迎来到 Lagent 的中文文档! — Lagent](https://lagent.readthedocs.io/zh-cn/latest/)

* **agents** 实现了多种智能体，如 ReAct，AutoGPT。
* **llms** 支持多种大语言模型，包括在 HuggingFace 上托管的开源模型（Llama-2, InterLM）及 GPT3.5/4 等闭源模型。
* **actions** 包含一系列工具，并提供工具执行器来统一管理。

目前看起来还是比较简略的。

远远比不上autogen

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711542922583-e94fe10e-09c3-4bac-9581-73fda69235fe.png)

![](https://cdn.nlark.com/yuque/0/2024/png/33726064/1711542996969-8fdd5642-57d8-4188-9e73-00e8ce3d974b.png)
